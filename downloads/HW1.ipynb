{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frIrlirMGj-7"
   },
   "source": [
    "# Data Processing in Python + Intro to LLMs\n",
    "In this notebook, you will start with raw, messy text data from the Yelp Reviews Dataset, clean it using Python, and then use a pre-trained Large Language Model from Hugging Face to perform an analysis task. You will then explore the tradeoffs between different LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWXRm__aHF8Y"
   },
   "source": [
    "## Part 1: Data Processing [15 points]\n",
    "The first step in any NLP pipeline is to get and clean your data. Real-world text is messy, and preparing it properly is crucial for getting good results from any model. We will use [this dataset of unprocessed restaurant reviews](https://huggingface.co/datasets/shreyahavaldar/restaurant_reviews_unprocessed).\n",
    "\n",
    "First, let's load the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9057f6fFHFEl"
   },
   "outputs": [],
   "source": [
    "!pip install datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SLVRyYi3bjx"
   },
   "source": [
    "## 1.1 Loading a dataset from HuggingFace\n",
    "\n",
    "Read the documentation below and load the training set of the restaurant reviews dataset using huggingface's `load_dataset` function. Save your data as a pandas dataframe called `reviews_df`.\n",
    "\n",
    "Documentation: [Loading Datasets from HuggingFace](https://huggingface.co/docs/datasets/loading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyzyt7OL3jLi"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\"\"\"TODO: Load the dataset\"\"\"\n",
    "\n",
    "# Save as a pandas dataframe called reviews_df\n",
    "reviews_df = pd.DataFrame()\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2VNcAQWJLTE"
   },
   "source": [
    "## 1.2: Data Cleaning\n",
    "\n",
    "Write a Python function called clean_text that performs a series of cleaning operations. Then, apply this function to the `text` column of `reviews_df` to create a new column called `text_clean`.\n",
    "\n",
    "Your clean_text function must perform the following steps in order:\n",
    "\n",
    "1. Remove URLs: Find and remove all URL patterns (starting with `https`, or `www`). Replace URLs with \"`[url]`\"\n",
    "\n",
    "2. Strip Emails & Phones: Find and remove email addresses and common US phone number formats. Replace emails with \"`[email]`\" and numbers with \"`[phone]`\"\n",
    "\n",
    "3. Remove HTML: Remove any simple HTML tags like `<br>` or `<div>`.\n",
    "\n",
    "4. Normalize Whitespace: Replace multiple whitespace characters (spaces, tabs, newlines) with a single space and trim leading/trailing whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDPrLynHSNUA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "    1) replace URLs with [url]\n",
    "    2) replace emails with [email] and phones with [phone]\n",
    "    3) remove simple HTML tags\n",
    "    4) normalize whitespace\n",
    "    \"\"\"\n",
    "\n",
    "# Apply to reviews_df to create a column called text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sz0u-IbT0-I"
   },
   "source": [
    "## Part 2: Generating Text with an LLM [45 points]\n",
    "\n",
    "Now that we have clean data, we can start working with a real Language Model! First, let's install the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yzs_0If1UKDm"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch sentencepiece --quiet\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acBa_JRbVMZl"
   },
   "source": [
    "## 2.1: Loading a Local Language Model\n",
    "\n",
    "We'll use the `Qwen2-1.5B-Instruct` model. It's powerful enough to follow instructions but small enough to run quickly in Colab.\n",
    "\n",
    "**To speed things up, connect to a GPU**\n",
    "1. Click on Runtime -> Change runtime type.\n",
    "2. Under \"Hardware accelerator\" click T4 GPU\n",
    "3. Click Save. The notebook will restart and you will be connected!\n",
    "\n",
    "First, load the `Qwen/Qwen2-1.5B-Instruct` model as an `AutoModelForCausalLM`. Set `device_map=\"auto\"` to automatically use the GPU. Then, load the `Qwen/Qwen2-1.5B-Instruct` tokenizer as an `AutoTokenizer`.\n",
    "\n",
    "Here is some documentation to help you:\n",
    "- [Loading a model from HuggingFace](https://huggingface.co/docs/transformers/en/models)\n",
    "- [Using a Tokenizer](https://huggingface.co/docs/transformers/en/fast_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xvo18guOVQqW"
   },
   "outputs": [],
   "source": [
    "assert(torch.cuda.is_available())\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "1. Load the tokenizer, which prepares text for the model\n",
    "2. Load the model. set device_map=\"auto\" to automatically uses the GPU.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM1eW9bpVAix"
   },
   "source": [
    "## 2.2: Using a Tokenizer & Chat Template\n",
    "\n",
    "Modern chat models are not trained on plain text; they are trained on structured conversations. To get the best performance, we must format our prompts to match this structure. The `tokenizer.apply_chat_template()` method handles this for us automatically.\n",
    "\n",
    "For the Qwen2 model, the template looks like this:\n",
    "\n",
    "    <|im_start|>system\n",
    "    You are a helpful assistant.\n",
    "    <|im_end|>\n",
    "    <|im_start|>user\n",
    "    Hello, how are you?\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "\n",
    "Key Components:\n",
    "\n",
    "- Roles: Each part of the conversation is assigned a role (system, user, or assistant).\n",
    "  - system: Sets the high-level instruction or persona for the model (e.g., \"You are a helpful assistant\").\n",
    "  - user: Represents the instruction or question you are asking.\n",
    "  - assistant: This is where the model's response begins. We end our prompt here to signal to the model that it's its turn to \"speak.\"\n",
    "- Special Tokens: The <|im_start|> and <|im_end|> tokens are special markers that the model uses to understand where each role's message begins and ends.\n",
    "\n",
    "When we create a messages list in Python and pass it to `tokenizer.apply_chat_template()`, the tokenizer expertly builds this perfectly formatted string for us. This is the most reliable way to interact with chat models and ensures we get the best possible results.\n",
    "\n",
    "**Your Task:**\n",
    "Write two Python functions called `tokenize_prompt` and `decode_tokens`.\n",
    "1. `tokenize_prompt` takes a **list of chat messages** as input. The function should use the global `tokenizer` and return the resulting `input_ids` as a PyTorch tensor that has been moved to the correct device (e.g., the GPU).\n",
    "2. `decode_tokens` takes a PyTorch tensor of `input_ids` as input and returns the decoded text.\n",
    "\n",
    "Here is some documentation to help you:\n",
    "- [Using a chat template](https://huggingface.co/docs/transformers/main/en/chat_templating#using-applychattemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJGoJQ3EU9on"
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(messages: list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the model's chat template to a list of messages and tokenizes it.\n",
    "\n",
    "    Args:\n",
    "      messages: A list of dictionaries, e.g., [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "\n",
    "    TODO: implement this function to return a PyTorch tensor containing the formatted and tokenized input_ids. Ensure the tensor is on the same device as the model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def decode_tokens(token_ids: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    TODO: implement this function to decode a tensor of token IDs back into a string.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# --- Test your functions ---\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly food critic.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about the best restaurant in Philadelphia.\"}]\n",
    "token_ids = tokenize_prompt(test_messages)\n",
    "\n",
    "print(f\"Input Messages: {test_messages}\")\n",
    "print(f\"\\nOutput Tensor (Token IDs):\\n{token_ids}\")\n",
    "print(f\"\\nTensor is on device: {token_ids.device}\")\n",
    "\n",
    "decoded_text = decode_tokens(token_ids)\n",
    "print(f\"\\nDecoded Text:\\n'{decoded_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKFGBZNFarAb"
   },
   "source": [
    "## 2.3: Generate Text & Classify the Review\n",
    "\n",
    "**Your Task:**\n",
    "Create a function `classify_review` that takes a review text as input. Inside the function, you will build a **chat prompt** (using a system and a user message) and use the Qwen2 model to classify the restaurant as either \"**Recommend**\" or \"**Avoid**\". Your function should return only the model's final, single-word classification.\n",
    "\n",
    "Steps:\n",
    "1. Create the chat messages inside the function. This should be a list containing two dictionaries:\n",
    "    - A system message that sets the model's persona.\n",
    "    - A user message that contains clear, direct instructions along with the review_text itself.\n",
    "2. Tokenize the prompt by passing your list of messages to the `tokenize_prompt` function you wrote in the previous step.\n",
    "3. Generate a response from the model using the `model.generate()` method.\n",
    "4. Decode the output using the `decode_tokens` function you wrote to get the final string. Remember, the `model.generate()` method returns the entire prompt plus the new tokens!\n",
    "5. Return only the final, one-word classification (e.g., \"Recommend\").\n",
    "\n",
    "Here is some documentation to help you:\n",
    "- [Using a chat template](https://huggingface.co/docs/transformers/main/en/chat_templating#using-applychattemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G50-Gu5rag6W"
   },
   "outputs": [],
   "source": [
    "def classify_review(review_text: str) -> str:\n",
    "    \"\"\"\n",
    "    TODO: Use the Qwen2 model to classify a review as \"Recommend\" or \"Avoid\".\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# --- Classify the first 3 cleaned reviews ---\n",
    "for i in range(3):\n",
    "    review = reviews_df['text_clean'].iloc[i]\n",
    "    classification = classify_review(review)\n",
    "    print(f\"REVIEW {i+1}: {review}\")\n",
    "    print(f\"Classification: {classification}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU8KUzSW6L5Z"
   },
   "source": [
    "## 2.4: Generating a News Headline with Temperature\n",
    "\n",
    "While temperature has little effect on a simple classification, it's the most important parameter for controlling creativity. Next, you will create a function that generates a news headline for a restaurant, allowing you to see how temperature influences the model's output.\n",
    "\n",
    "**Your Task:**\n",
    "Create a function `generate_news_headline` that accepts a `review_text` and a `temperature`. You will use the Qwen2 model to generate a catchy news headline based on the review!\n",
    "\n",
    "Steps:\n",
    "1. Create the chat messages inside the function.\n",
    "    - A system message that sets the model's persona.\n",
    "    - A user message that contains clear, direct instructions along with the review_text itself.\n",
    "2. Tokenize the prompt using your tokenize_prompt function.\n",
    "3. Generate a response using `model.generate()` and include the `temperature` parameter. You must also include `do_sample=True` in this call, as temperature has no effect without it.\n",
    "4. Decode, process, and return the final headline string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHxHPHg2581w"
   },
   "outputs": [],
   "source": [
    "def generate_news_headline(review_text: str, temperature: float) -> str:\n",
    "    \"\"\"\n",
    "    TODO: Generate a catchy news headline based on a review, using a specific\n",
    "    temperature to control creativity.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Choosing the Right LLM [20 points]\n",
    "\n",
    "In Part 2 we interacted with a single checkpoint. Here we extend that code to compare multiple HuggingFace models on the same downstream task so we can explore which model is the best for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build an evaluation framework [10 points]\n",
    "\n",
    "We will evaluate on five short reviews. Implement helper functions that:\n",
    "\n",
    "1. Load a tokenizer + model given a HuggingFace model name.\n",
    "2. Use the loaded tokenizer + model to classify a cleaned review as either \"Recommend\" or \"Avoid\"\n",
    "\n",
    "\ud83d\udca1 *Hint:* Copy/paste your helpers from Part 2. The only new code you need is a thin wrapper that swaps in whatever tokenizer/model `evaluate_model` passes you. \n",
    "\n",
    "We've already implemented the `evaluate_model` helper that loops over the eval set, times each prediction, and builds a `DataFrame` for evaluation metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "EVAL_REVIEWS = [\n",
    "    {\"review_id\": \"r1\", \"text_clean\": \"The tacos were incredible\u2014handmade tortillas, bright salsa, and attentive staff.\", \"label\": \"Recommend\"},\n",
    "    {\"review_id\": \"r2\", \"text_clean\": \"We waited almost two hours, the pasta arrived cold, and no one apologized.\", \"label\": \"Avoid\"},\n",
    "    {\"review_id\": \"r3\", \"text_clean\": \"Light, fluffy pancakes with real maple syrup made this brunch worth the trip.\", \"label\": \"Recommend\"},\n",
    "    {\"review_id\": \"r4\", \"text_clean\": \"Greasy tables, rude service, and a stomach ache afterward\u2014never again.\", \"label\": \"Avoid\"},\n",
    "    {\"review_id\": \"r5\", \"text_clean\": \"Cozy ramen bar with rich broth, fresh noodles, and fair prices.\", \"label\": \"Recommend\"},\n",
    "]\n",
    "\n",
    "\n",
    "def load_model_components(model_name: str):\n",
    "    \"\"\"TODO: Load and return a tokenizer + model for the provided checkpoint.\"\"\"\n",
    "\n",
    "\n",
    "def classify_with_model(model, tokenizer, review_text: str) -> str:\n",
    "    \"\"\"TODO: Format a prompt, run generation, and return either 'Recommend' or 'Avoid'.\"\"\"\n",
    "\n",
    "\n",
    "def evaluate_model(model_name: str, max_samples: int = len(EVAL_REVIEWS)) -> pd.DataFrame:\n",
    "    \"\"\"Runs the eval set through the given model and returns a DataFrame of metrics.\"\"\"\n",
    "    tokenizer, model = load_model_components(model_name)\n",
    "    rows = []\n",
    "    for sample in EVAL_REVIEWS[:max_samples]:\n",
    "        start = time.perf_counter()\n",
    "        prediction = classify_with_model(model, tokenizer, sample[\"text_clean\"])\n",
    "        latency = time.perf_counter() - start\n",
    "        rows.append({\n",
    "            \"review_id\": sample[\"review_id\"],\n",
    "            \"model\": model_name,\n",
    "            \"prediction\": prediction,\n",
    "            \"gold_label\": sample[\"label\"],\n",
    "            \"correct\": prediction == sample[\"label\"],\n",
    "            \"latency_sec\": latency,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare three chat-tuned models [10 points]\n",
    "\n",
    "Now, let's compare the three different chat/instruction-tuned checkpoints listed below. For each model:\n",
    "\n",
    "1. Run `evaluate_model(model_name)`.\n",
    "2. Display the resulting DataFrame.\n",
    "3. Print accuracy and average latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = [\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",\n",
    "]\n",
    "\n",
    "for model_name in candidate_models:\n",
    "    \"\"\"TODO: Call evaluate_model for this checkpoint and display the results\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1RU4gf4iRPGv4KpI-7IIZpNZoQVzWYZ5z",
     "timestamp": 1758206799602
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}